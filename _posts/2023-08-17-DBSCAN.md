---
layout: post
title:  "DBSCAN"
date:   2023-08-17
author: Naomi Zubeldia
description: A brief description of what DBSCAN is and how it works
image: /assets/images/image5.jpg
---
### What is "DBSCAN"?
The *density-based spatial clustering of applications with noise* (DBSCAN) is an unsupervised machine algorithm 
that works on identifying different clusters of data. There are many other clustering algorithms out there 
that have different ways to identify clusters. DBSCAN is specifically good at working with clusters that are 
nested together and not being influenced by outliers in its computations. Also, it does not require the user 
to set the number of desired clusters beforehand. Below is an image comparing DBSCAN to one of the more 
popular clutsering techniques, K-means. You can see that DBSCAN is able to identify the nested clusters better 
than K-means.
<img src="https://user-images.githubusercontent.com/7659/74451662-d2325000-4e34-11ea-9770-a57e81259eb9.png" alt= "Kmeans vs DBSCAN" width="500" height="300">
DBSCAN works by grouping points based on the high density of points nearby. You set parameters like a perimeter size around each point and a count of how many points are in the perimeter to help with point grouping. The different types of points are:

**Core Points**: this is any point that has more than or equal to the pre-set count within its perimeter.

**Border Points**: this is any point that does not have enough points in its perimeter to be considered a Core Point but is within a core point's perimeter

**Outlier Points**: this is any point that does not have enough points in its perimeter and is not in any core point's perimeter, which would be considered a low density area.

The algorithm then starts creating clusters of core points. It starts with a random point and clusters all core points together. It then adds any border points near by to the cluster. It will do this process for other clusters until there are just outlier points left.

<img src="https://miro.medium.com/max/627/1*yT96veo7Zb5QeswV7Vr7YQ.png" alt= "Points" width="500" height="300">
I would say that the algorithm does struggle a bit with border points that are close to two different clusters. DBSCAN works sequentially so it will set those close border points to the first cluster that is worked on when that may not be the correct cluster for each point. Otherwise, it is a very powerful alorithm that is capable of identifying clusters in most shapes and pretty robust to outliers. 
### The steps of DBSCAN
**1.** The DBSCAN algorithm starts by defining two main parameters:
the radius of the `neighborhood` (eps), which is the perimeter for each point, and the `minimum number of points required to form a dense region` (min_samples) in that perimeter. The value of eps sets the area around each point that we use to locate other points. Any point within eps distance from a given point is considered to be in its "neighborhood". The parameter min_samples sets the minimum number of points in a neighborhood to be required for a point to be considered a core point.

**2.** For each data point, the algorithm checks how many other points are within its eps neighborhood. If the number of points in the neighborhood meets the min_samples parameter, the point is considered a core point. Otherwise, it is classified as a border point.
<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/db10.png" alt="Core point" width="230" height="230">

**3.** Starting from a random core point, the algorithm builds a cluster by adding all the core points within its eps neighborhood to the same cluster. It then proceeds to add all the core points within the neighborhood of the newly added points until there are no more core points to add to the cluster.
<img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2020/03/db12.png" alt="Clustering" width="230" height="230">

**4.** Once all the core points in a cluster have been added, the algorithm checks the border points in the neighborhood of the cluster to see if any of them can be added to the same cluster. This process continues until there are no more points to be added to the cluster.

**5.** The algorithm repeats the process for all the core points that have not been assigned to a cluster. Any points that do not belong to any cluster are considered the outliers.
<img src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*nZDyFlpq7X_k0zEYYAKfHg.png" alt="outlier" width="230" height="230">

By applying these steps to the data, DBSCAN can identify clusters of arbitrary shapes and sizes, as well as detect and remove noise. DBSCAN is a good algorithm to use when you have weirdly shaped and nested clusters that you want to identify when you do not know how many clusters there are.
Unfortunately, DBSCAN does not have a predict method. It can predict labels of data but that will be an unsupervised prediction. It's primary purpose is to just set clusters and detect outliers. You will have to use an actual classifcation method to do any predictions.
### EXAMPLE
To better understand how DBSCAN works, I provided a follow along example with the simple moons data set you can get from the sklearn. You will first need the following packages to run the code and read in the moons data including importing the `Scikit-Cluster` package `DBSCAN`.
```
import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
```
```
Xm, ym = make_moons(1000, noise=0.1, random_state=100)
plt.scatter(Xm[:,0], Xm[:,1], c=ym)
```
The moons data set is a simple one that has a bit of nesting between the two clusters. I choose this data to show how DBSCAN works well with nested data.

To read in DBSCAN, you just need to call it and enter in the epsilon and min_sampes parameters like seen below.
```
cluster= DBSCAN(eps=.1,min_samples=8)
cluster.fit(Xm,ym)
```
#### Hypertuning the parameters
Since the parameters of DBSCAN are up to us to set, we need a way to find the *best ones* for the algorithm to recognize the different clusters. There is not really a hypertuner for DBSCAN parameters, which would leave us to just testing so many different combinations by hand--Gross!
###### Finding Epsilon
However, we are able to use sklearn's Nearest Neighbors package to help us determine what a good size for epsilon is based on how many neighbors/observations we want to be near each point on average. 

After running the data through NearestNeighbors, we can create a distance matrix to then run through the `KneeLocator` function, which requires you have the package `kneed`, that then plots all the distances. From that plot, it can find the "knee" or the point of the distances where they start to increase. At that point, there is the distance that will find the amount of neighbors we want.
```
#install the kneed library
%pip install -q -U kneed
```
```
from sklearn.neighbors import NearestNeighbors
k=20 #change this to try different neighbors

neigh = NearestNeighbors(n_neighbors=k)
nbrs = neigh.fit(Xm)
distances, indices = nbrs.kneighbors(Xm)#creates the distances matrix
sorted_distances = np.sort(distances[:,k-1])#sorts the distances and identifies the farthest one
```
```
# Running the KneeLocator


from kneed import KneeLocator

i = np.arange(len(sorted_distances))

#takes the sorted distances and finds where the distances curve at the S rate of 1, which is how fast it finds the curve
knee = KneeLocator(i, sorted_distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')

#plots the distances curve with where the knee point it
fig = plt.figure(figsize=(5, 5))
knee.plot_knee()
plt.xlabel("Points")
plt.ylabel("Distance")

#gives the exact estimate of the knee
print(sorted_distances[knee.knee])
```
There is not an exact way to find the best k to use for the knee locator, unfortunately, other than through trial and error. I tried a couple of different numbers for the k and found 20 to be the best to use. I am not sure if there is an exact way to choose the k but it is easier to input whole numbers rather going through all possible values for epsilon between 0-1.
###### Finding min_samples
After finding the best epsilon, the min_samples is found in a more "arbitary" way. The min_samples should be greater than the dimensionality of the dataset as well as sort of close to what k you used for the kneelocator. Usually for 2 dimensional data, you should use the default 4 or greater. So, I started out with a min_samples of 4, which was not good. I tried 5-8 and did not find much of a difference, so I kept it at 5 since that was close to the default.

*(If your data has a high dimensionality, you should consider another type of clustering that can deal with it because DBSCAN does not do well with high dimensions.)* 
### Fitting the best model
After trying a couple of different k's and min_samples, I found the best parameters to be eps=.161 and min_samples=5. The DBCAN found there to be two clusters with only 2 outliers.
```
cluster= DBSCAN(eps=.161,min_samples=5)
cluster.fit(Xm,ym)
```
```
plt.scatter(Xm[:, 0], 
            Xm[:, 1], 
            c=cluster.labels_, 
            label=ym)

labels=cluster.labels_
N_clus=len(set(labels))-(1 if -1 in labels else 0)
print('Estimated no. of clusters: %d' % N_clus)

n_noise = list(cluster.labels_).count(-1)
print('Estimated no. of noise points: %d' % n_noise)
```
### So, what to do with ANOTHER clustering method?
It may feel overwhelming with what method to choose for clustering since there are *so* many of them. You need to consider what you data is like and what each method is like before you choose one. 

DBSCAN brings advantages of not needing to know the number of clusters that you have beforehand and it is very good at identifying randomly shaped and nested clusters. You also need to consider that it depends on the values of epsilon and min_samples are up to you choose and may take time to find. Also, DBSCAN is uses the density of the points to determine clusters. If you have data that the density of the points is where the clusters are close together, then DBSCAN will struggle identifying them. 

So, DBSCAN may not be the algorthim you go to every time but if you unsure about the number of clusters and the outliers in the data then it will be a good option to use! Here is a website to reference about different clustering methods: [discovering clustering](https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/). Good luck in your clustering!!



